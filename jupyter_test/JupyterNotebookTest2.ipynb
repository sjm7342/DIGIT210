{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6870ea6-aed3-4253-ad8e-e950bbde81f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'request' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m file= \u001b[33m\"\u001b[39m\u001b[33mavatarSpeeches.py\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m response = \u001b[43mrequest\u001b[49m.urlopen(file)\n\u001b[32m      3\u001b[39m br = response.read().decode(\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mtype\u001b[39m(br)\n",
      "\u001b[31mNameError\u001b[39m: name 'request' is not defined"
     ]
    }
   ],
   "source": [
    "file= \"avatarSpeeches.py\"\n",
    "response = request.urlopen(file)\n",
    "br = response.read().decode('utf8')\n",
    "type(br)\n",
    "print(len(br))\n",
    "# make a variable\n",
    "howLong = len(br)\n",
    "# picture string version! \n",
    "print(f\"howLong = {howLong}\")\n",
    "novelSlice = br[:500]\n",
    "print(f\"novelSlice = {novelSlice}\")\n",
    "\n",
    "splitEmUp = br.split()\n",
    "print(f\"splitEmUp = {splitEmUp[-100:]}\")\n",
    "\n",
    "for token in splitEmUp:\n",
    "    if token.endswith('ing'):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5be9ebdb-2e72-454a-a0b9-76f1500371ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'request'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequest\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'request'"
     ]
    }
   ],
   "source": [
    "import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f006b5a4-cc6d-4d03-bcac-ba88997a2e52",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DIGIT210/jupyter_test/avatarSpeeches.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDIGIT210/jupyter_test/avatarSpeeches.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     data = f.read()\n\u001b[32m      4\u001b[39m tokens = word_tokenize(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'DIGIT210/jupyter_test/avatarSpeeches.py'"
     ]
    }
   ],
   "source": [
    "with open(\"DIGIT210/jupyter_test/avatarSpeeches.py\", 'r', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d55a18d-fe25-4ef4-a6c8-d353c83a09c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'jupyter_test/avatarSpeeches.py'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjupyter_test/avatarSpeeches.py\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     data = f.read()\n\u001b[32m      4\u001b[39m tokens = word_tokenize(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'jupyter_test/avatarSpeeches.py'"
     ]
    }
   ],
   "source": [
    "with open(\"jupyter_test/avatarSpeeches.py\", 'r', encoding='utf8') as f:\n",
    "    data = f.read()\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce0d9e8-0d58-4716-8a49-f4433578c7a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3616775978.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwith open(\"DIGIT210/jupyter_test/avatarSpeeches.py\",)\u001b[39m\n                                                         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "with open(\"DIGIT210/jupyter_test/avatarSpeeches.py\",)\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4dc6437-d1f7-4ffc-93ee-1d9679048334",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (1003032886.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwith open(\"DIGIT210/jupyter_test/avatarSpeeches.py\")\u001b[39m\n                                                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "with open(\"DIGIT210/jupyter_test/avatarSpeeches.py\")\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35a751dc-4c4c-4e06-a4f3-f655b5911f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'avatarSpeeches.py'\n",
    "f = open(filepath, 'r', encoding='utf8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b2e0d6c-e9a6-4329-9c1b-1ef409bd89bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (3386809424.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwith open(\"avatarSpeeches.py\")\u001b[39m\n                                  ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m expected ':'\n"
     ]
    }
   ],
   "source": [
    "with open(\"avatarSpeeches.py\")\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f16b249-dbbe-4633-958b-91728f110a2d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3904959815.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mwith open 'DIGIT210/jupyter_test/avatarSpeeches.py'\u001b[39m\n              ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "with open 'DIGIT210/jupyter_test/avatarSpeeches.py'\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89abd1f5-15cb-465f-a2d8-20db1edfbdba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m filepath = \u001b[33m'\u001b[39m\u001b[33mavatarSpeeches.py\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m).read()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokens = \u001b[43mword_tokenize\u001b[49m(data)\n",
      "\u001b[31mNameError\u001b[39m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "filepath = 'avatarSpeeches.py'\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "\n",
    "tokens = word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80a315b7-7c33-4193-a351-a58a9141da5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m filepath = \u001b[33m'\u001b[39m\u001b[33mavatarSpeeches.py\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      2\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m).read()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m tokenList = \u001b[43mword_tokenize\u001b[49m(f)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenList)\n",
      "\u001b[31mNameError\u001b[39m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "filepath = 'avatarSpeeches.py'\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "\n",
    "tokenList = word_tokenize(f)\n",
    "print(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d96c5132-5aaf-41ac-8129-4df42a0fc339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d41122e-6555-4606-acf9-5c3b1b535dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56bbd6c8-62b4-4f20-b12f-479a04917508",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('aang'):\n",
    "    print(synset, \": \", synset.lemma_names(), \": \", len(synset.lemma_names()))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "427e29ff-b476-4c09-b91f-ba7d7c12a0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'merchant'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.morphy('merchants')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5bb673e-79f7-4ff1-a808-ab3b2484c86e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/seanmartin/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/share/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m filepath = \u001b[33m'\u001b[39m\u001b[33mavatarSpeeches.py\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      6\u001b[39m f = \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf8\u001b[39m\u001b[33m'\u001b[39m).read()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m tokenList = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenList)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/seanmartin/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/share/nltk_data'\n    - '/opt/homebrew/Cellar/jupyterlab/4.4.0/libexec/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "filepath = 'avatarSpeeches.py'\n",
    "f = open(filepath, 'r', encoding='utf8').read()\n",
    "tokenList = word_tokenize(f)\n",
    "print(tokenList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8f6d0f9b-c659-4d0d-a02b-4d1f04855620",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m lowercaseTokens = [token.lower() \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtokenList\u001b[49m]\n\u001b[32m      2\u001b[39m uniqueTokens = \u001b[38;5;28mset\u001b[39m(lowercaseTokens)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(uniqueTokens)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenList' is not defined"
     ]
    }
   ],
   "source": [
    "lowercaseTokens = [token.lower() for token in tokenList]\n",
    "uniqueTokens = set(lowercaseTokens)\n",
    "print(uniqueTokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "152b531c-b90e-421f-ac28-28b91c7cbdbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (4009941455.py, line 8)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mif synsets:\u001b[39m\n               ^\n\u001b[31m_IncompleteInputError\u001b[39m\u001b[31m:\u001b[39m incomplete input\n"
     ]
    }
   ],
   "source": [
    "for w in sorted(shortList):\n",
    "    lemma = wn.morphy(w)\n",
    "    # I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \n",
    "    lemma = lemma if lemma else w \n",
    "    print(f\"Word: {w} | Wordnet Lemma: {lemma}\")\n",
    "    synsets = wn.synsets(lemma)\n",
    "    pos = {synset.pos() for synset in synsets}\n",
    "    if synsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13921b9e-c580-4f43-97a0-5a274ab95dc8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shortList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mshortList\u001b[49m):\n\u001b[32m      2\u001b[39m     lemma = wn.morphy(w)\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'shortList' is not defined"
     ]
    }
   ],
   "source": [
    "for w in sorted(shortList):\n",
    "    lemma = wn.morphy(w)\n",
    "    # I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \n",
    "    lemma = lemma if lemma else w \n",
    "    print(f\"Word: {w} | Wordnet Lemma: {lemma}\")\n",
    "    synsets = wn.synsets(lemma)\n",
    "    pos = {synset.pos() for synset in synsets}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17501ccd-8381-4899-b77c-b5c67bf0ee13",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coll' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os.listdir(\u001b[43mcoll\u001b[49m):\n\u001b[32m      2\u001b[39m    \u001b[38;5;28;01mif\u001b[39;00m file.endswith(\u001b[33m\"\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      3\u001b[39m         filepath = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcoll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'coll' is not defined"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(coll):\n",
    "   if file.endswith(\".py\"):\n",
    "        filepath = f\"{coll}/{file}\"\n",
    "        print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed2ad6fb-aafa-4b6d-b5de-abf78e6f2dc3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shortList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(\u001b[43mshortList\u001b[49m):\n\u001b[32m      2\u001b[39m     lemma = wn.morphy(w)\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'shortList' is not defined"
     ]
    }
   ],
   "source": [
    "for w in sorted(shortList):\n",
    "    lemma = wn.morphy(w)\n",
    "    # I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \n",
    "    lemma = lemma if lemma else w \n",
    "    print(f\"Word: {w} | Wordnet Lemma: {lemma}\")\n",
    "    synsets = wn.synsets(lemma)\n",
    "    pos = {synset.pos() for synset in synsets}\n",
    "    if synsets:\n",
    "       \n",
    "        \n",
    "        print(f\" Word: {w}, POS-according-to-WordNet {pos} Number of Synsets (Ambiguity): {len(synsets)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "945ff5b5-e803-4bce-9d7a-35033b894c3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shortList'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshortList\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(shortList):\n\u001b[32m      4\u001b[39m     lemma = wn.morphy(w)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'shortList'"
     ]
    }
   ],
   "source": [
    "import shortList\n",
    "\n",
    "for w in sorted(shortList):\n",
    "    lemma = wn.morphy(w)\n",
    "    # I don't think we need the next line, but it's a fallback if there's no WordNet lemmas: \n",
    "    lemma = lemma if lemma else w \n",
    "    print(f\"Word: {w} | Wordnet Lemma: {lemma}\")\n",
    "    synsets = wn.synsets(lemma)\n",
    "    pos = {synset.pos() for synset in synsets}\n",
    "    if synsets:\n",
    "       \n",
    "        \n",
    "        print(f\" Word: {w}, POS-according-to-WordNet {pos} Number of Synsets (Ambiguity): {len(synsets)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96591a4d-4ce7-4b8b-a7fa-c77725b14a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('aang'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9805e320-44cb-44da-8e31-5211a075a75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['spider'] : Part of Speech:  n : Definition:  predatory arachnid with eight legs, two poison fangs, two feelers, and usually two silk-spinning organs at the back end of the body; they spin silk to make cocoons for eggs or traps for prey\n",
      "['spider', 'wanderer'] : Part of Speech:  n : Definition:  a computer program that prowls the internet looking for publicly accessible resources that can be added to a database; the database can then be searched with a search engine\n",
      "['spider'] : Part of Speech:  n : Definition:  a skillet made of cast iron\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('spider'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56eb3c52-80a5-418e-b9c7-8f1ba1a3c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mountain', 'mount'] : Part of Speech:  n : Definition:  a land mass that projects well above its surroundings; higher than a hill\n",
      "['batch', 'deal', 'flock', 'good_deal', 'great_deal', 'hatful', 'heap', 'lot', 'mass', 'mess', 'mickle', 'mint', 'mountain', 'muckle', 'passel', 'peck', 'pile', 'plenty', 'pot', 'quite_a_little', 'raft', 'sight', 'slew', 'spate', 'stack', 'tidy_sum', 'wad'] : Part of Speech:  n : Definition:  (often followed by `of') a large number or amount or extent\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('mountain'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b798ff2-bcbd-40c0-8829-abfdd3ac2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for synset in wn.synsets('the'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "085704d5-1936-4d09-aed3-94dfe5cfbce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wire'] : Part of Speech:  n : Definition:  ligament made of metal and used to fasten things or make cages or fences etc\n",
      "['wire', 'conducting_wire'] : Part of Speech:  n : Definition:  a metal conductor that carries electricity over a distance\n",
      "['wire'] : Part of Speech:  n : Definition:  the finishing line on a racetrack\n",
      "['telegram', 'wire'] : Part of Speech:  n : Definition:  a message transmitted by telegraph\n",
      "['wire'] : Part of Speech:  v : Definition:  provide with electrical circuits\n",
      "['cable', 'telegraph', 'wire'] : Part of Speech:  v : Definition:  send cables, wires, or telegrams\n",
      "['wire'] : Part of Speech:  v : Definition:  fasten with wire\n",
      "['wire'] : Part of Speech:  v : Definition:  string on a wire\n",
      "['electrify', 'wire'] : Part of Speech:  v : Definition:  equip for use with electricity\n"
     ]
    }
   ],
   "source": [
    "for synset in wn.synsets('wire'):\n",
    "    print(synset.lemma_names(), \": Part of Speech: \", synset.pos(), \": Definition: \", synset.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1790b2-3452-49c1-938d-2e812d7cc494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
